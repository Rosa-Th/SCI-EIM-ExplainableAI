{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a value function accounting for feature independence that is undealt with by SHAP explanation (Unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'High', 1: 'Low', 2: 'Medium'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"../preprocessed/clean_survey.csv\", index_col=False)\n",
    "# split to indenpendent and dependent variables\n",
    "# drop all variables computing the labels\n",
    "X = df.drop(['q_031_adjusted', 'invol_score', 'q_031', 'involvement_level'] + [f'q_0{i}' for i in range(45, 55)], axis=1)\n",
    "y = df[\"involvement_level\"]\n",
    "\n",
    "# Encode the class labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_encoded_train, y_encoded_test = train_test_split(X, y_encoded, random_state=42)\n",
    "\n",
    "# Data Preprocessing\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Store in dataframe for plotting\n",
    "feature_names = X_train.columns.tolist()\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "label_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Subset Accuracy:  0.6438356164383562\n",
      "Hamming Loss: 0.3561643835616438\n",
      "F1 Score (Micro): 0.6438356164383562\n",
      "F1 Score (Macro): 0.6121799081551403\n",
      "F1 Score (Weighted): 0.6433986783111004\n",
      "ROC-AUC (Macro): 0.8132172353492225\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# fit the best model\n",
    "# Load the model from a file\n",
    "with open('../models/neural_1.pkl', 'rb') as file:\n",
    "    model1 = pickle.load(file)\n",
    "\n",
    "model1.fit(X_train_scaled, y_encoded_train)\n",
    "# Making predictions\n",
    "y_pred1 = model1.predict(X_test_scaled)\n",
    "\n",
    "accuracy1 = accuracy_score(y_encoded_test, y_pred1)\n",
    "print(\"Testing Subset Accuracy: \", accuracy1)\n",
    "print(\"Hamming Loss:\", hamming_loss(y_encoded_test, y_pred1))\n",
    "\n",
    "# Micro-average\n",
    "print(\"F1 Score (Micro):\", f1_score(y_encoded_test, y_pred1, average='micro'))\n",
    "# Macro-average\n",
    "print(\"F1 Score (Macro):\", f1_score(y_encoded_test, y_pred1, average='macro'))\n",
    "# Weighted-average\n",
    "print(\"F1 Score (Weighted):\", f1_score(y_encoded_test, y_pred1, average='weighted'))\n",
    "\n",
    "# ROC-AUC\n",
    "# Note: roc_auc_score expects a shape of (n_samples,) for binary problems and\n",
    "# (n_samples, n_classes) for multi-class/multi-label with one-hot encoding.\n",
    "# ROC-AUC calculation needs probabilistic predictions `y_proba`\n",
    "# use macro since class labels are imbalanced\n",
    "y_proba = model1.predict_proba(X_test_scaled)\n",
    "roc_auc = roc_auc_score(y_encoded_test, y_proba, average='macro', multi_class='ovr')\n",
    "print(\"ROC-AUC (Macro):\", roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def custom_value_function(model, baseline_data, feature_indices, interaction_indices):\n",
    "    \"\"\"\n",
    "    Compute the value of the model prediction with and without the interaction terms.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained machine learning model.\n",
    "    - baseline_data: Data used as the baseline (background) dataset.\n",
    "    - feature_indices: Indices of the primary features to evaluate.\n",
    "    - interaction_indices: Indices of the features to interact with the primary features.\n",
    "    \n",
    "    Returns:\n",
    "    - Value difference caused by the interaction.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create a copy of the baseline data to manipulate\n",
    "    data_with_interaction = baseline_data.copy()\n",
    "    data_without_interaction = baseline_data.copy()\n",
    "    \n",
    "    # Set interaction features to their mean or a neutral value in data_without_interaction\n",
    "    for idx in interaction_indices:\n",
    "        data_without_interaction.iloc[:, idx] = baseline_data.iloc[:, idx].mean()\n",
    "    \n",
    "    # Predict with and without interaction\n",
    "    prediction_with_interaction = model.predict_proba(data_with_interaction.values)[:, 1]\n",
    "    prediction_without_interaction = model.predict_proba(data_without_interaction.values)[:, 1]\n",
    "    \n",
    "    # Calculate the difference due to interaction\n",
    "    value_difference = np.mean(prediction_with_interaction - prediction_without_interaction)\n",
    "    \n",
    "    return value_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 219 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import itertools\n",
    "def compute_interaction_effects(model, X, feature_names):\n",
    "    # Initialize SHAP explainer on the model\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, X)\n",
    "\n",
    "    # Dictionary to store interaction effects\n",
    "    interaction_effects = {}\n",
    "\n",
    "    # Calculate interactions between all pairs of features\n",
    "    for feat_a, feat_b in itertools.combinations(feature_names, 2):\n",
    "        idx_a = X.columns.get_loc(feat_a)\n",
    "        idx_b = X.columns.get_loc(feat_b)\n",
    "\n",
    "        # Calculate the interaction effect\n",
    "        interaction_value = custom_value_function(model, X, [idx_a], [idx_b])\n",
    "        interaction_effects[(feat_a, feat_b)] = interaction_value\n",
    "    \n",
    "    return interaction_effects\n",
    "\n",
    "# Example usage\n",
    "interaction_effects = compute_interaction_effects(model1, X_test_scaled_df, ['q_030', 'q_021', 'q_025'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot interaction effects\n",
    "plt.figure(figsize=(10, 6))\n",
    "names, values = zip(*interaction_effects.items())\n",
    "plt.barh(range(len(names)), values, tick_label=[f\"{a} x {b}\" for a, b in names])\n",
    "plt.xlabel('Interaction Effect Value')\n",
    "plt.title('Interaction Effects Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying SHAP, do a correlation analysis to find highly interacting feature pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_000</th>\n",
       "      <th>q_001</th>\n",
       "      <th>q_003</th>\n",
       "      <th>q_004</th>\n",
       "      <th>q_005</th>\n",
       "      <th>q_006</th>\n",
       "      <th>q_007</th>\n",
       "      <th>q_008</th>\n",
       "      <th>q_009</th>\n",
       "      <th>q_011</th>\n",
       "      <th>...</th>\n",
       "      <th>q_125_none</th>\n",
       "      <th>q_125_other</th>\n",
       "      <th>q_130_ACT</th>\n",
       "      <th>q_130_NSW</th>\n",
       "      <th>q_130_NT</th>\n",
       "      <th>q_130_QLD</th>\n",
       "      <th>q_130_SA</th>\n",
       "      <th>q_130_TAS</th>\n",
       "      <th>q_130_VIC</th>\n",
       "      <th>q_130_WA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q_000</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049903</td>\n",
       "      <td>0.256543</td>\n",
       "      <td>0.167943</td>\n",
       "      <td>0.226024</td>\n",
       "      <td>0.212825</td>\n",
       "      <td>0.189307</td>\n",
       "      <td>0.280651</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.209196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096321</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.074067</td>\n",
       "      <td>0.083569</td>\n",
       "      <td>0.025828</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.051621</td>\n",
       "      <td>0.047694</td>\n",
       "      <td>0.159945</td>\n",
       "      <td>0.036922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_001</th>\n",
       "      <td>0.049903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108317</td>\n",
       "      <td>0.057994</td>\n",
       "      <td>0.021039</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.015211</td>\n",
       "      <td>0.013852</td>\n",
       "      <td>0.051934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028142</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.046457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_003</th>\n",
       "      <td>0.256543</td>\n",
       "      <td>0.108317</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214660</td>\n",
       "      <td>0.264891</td>\n",
       "      <td>0.161847</td>\n",
       "      <td>0.190428</td>\n",
       "      <td>0.180217</td>\n",
       "      <td>0.280224</td>\n",
       "      <td>0.237975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105369</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>0.054312</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>0.040850</td>\n",
       "      <td>0.153212</td>\n",
       "      <td>0.055473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_004</th>\n",
       "      <td>0.167943</td>\n",
       "      <td>0.057994</td>\n",
       "      <td>0.214660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170764</td>\n",
       "      <td>0.167594</td>\n",
       "      <td>0.122863</td>\n",
       "      <td>0.215559</td>\n",
       "      <td>0.139821</td>\n",
       "      <td>0.122239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082332</td>\n",
       "      <td>0.036288</td>\n",
       "      <td>0.036437</td>\n",
       "      <td>0.037971</td>\n",
       "      <td>0.044416</td>\n",
       "      <td>0.106358</td>\n",
       "      <td>0.066461</td>\n",
       "      <td>0.079120</td>\n",
       "      <td>0.072732</td>\n",
       "      <td>0.063724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_005</th>\n",
       "      <td>0.226024</td>\n",
       "      <td>0.021039</td>\n",
       "      <td>0.264891</td>\n",
       "      <td>0.170764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220608</td>\n",
       "      <td>0.153931</td>\n",
       "      <td>0.175262</td>\n",
       "      <td>0.159404</td>\n",
       "      <td>0.233553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.042583</td>\n",
       "      <td>0.019635</td>\n",
       "      <td>0.029016</td>\n",
       "      <td>0.042271</td>\n",
       "      <td>0.064769</td>\n",
       "      <td>0.076097</td>\n",
       "      <td>0.047702</td>\n",
       "      <td>0.092577</td>\n",
       "      <td>0.015221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_130_QLD</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.106358</td>\n",
       "      <td>0.064769</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>0.043102</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.067799</td>\n",
       "      <td>0.067677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035946</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>0.062548</td>\n",
       "      <td>0.371575</td>\n",
       "      <td>0.096443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.163595</td>\n",
       "      <td>0.096443</td>\n",
       "      <td>0.269957</td>\n",
       "      <td>0.157226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_130_SA</th>\n",
       "      <td>0.051621</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>0.066461</td>\n",
       "      <td>0.076097</td>\n",
       "      <td>0.060867</td>\n",
       "      <td>0.023131</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.016460</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>0.036383</td>\n",
       "      <td>0.216135</td>\n",
       "      <td>0.056098</td>\n",
       "      <td>0.163595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056098</td>\n",
       "      <td>0.157026</td>\n",
       "      <td>0.091454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_130_TAS</th>\n",
       "      <td>0.047694</td>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.040850</td>\n",
       "      <td>0.079120</td>\n",
       "      <td>0.047702</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.029653</td>\n",
       "      <td>0.068186</td>\n",
       "      <td>0.065537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.021448</td>\n",
       "      <td>0.127416</td>\n",
       "      <td>0.033071</td>\n",
       "      <td>0.096443</td>\n",
       "      <td>0.056098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092570</td>\n",
       "      <td>0.053914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_130_VIC</th>\n",
       "      <td>0.159945</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.153212</td>\n",
       "      <td>0.072732</td>\n",
       "      <td>0.092577</td>\n",
       "      <td>0.127569</td>\n",
       "      <td>0.124956</td>\n",
       "      <td>0.210926</td>\n",
       "      <td>0.241464</td>\n",
       "      <td>0.138426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077269</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>0.060037</td>\n",
       "      <td>0.356655</td>\n",
       "      <td>0.092570</td>\n",
       "      <td>0.269957</td>\n",
       "      <td>0.157026</td>\n",
       "      <td>0.092570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_130_WA</th>\n",
       "      <td>0.036922</td>\n",
       "      <td>0.046457</td>\n",
       "      <td>0.055473</td>\n",
       "      <td>0.063724</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.083530</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.074990</td>\n",
       "      <td>0.062237</td>\n",
       "      <td>0.013202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020095</td>\n",
       "      <td>0.020095</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.207721</td>\n",
       "      <td>0.053914</td>\n",
       "      <td>0.157226</td>\n",
       "      <td>0.091454</td>\n",
       "      <td>0.053914</td>\n",
       "      <td>0.150913</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              q_000     q_001     q_003     q_004     q_005     q_006  \\\n",
       "q_000      1.000000  0.049903  0.256543  0.167943  0.226024  0.212825   \n",
       "q_001      0.049903  1.000000  0.108317  0.057994  0.021039  0.008038   \n",
       "q_003      0.256543  0.108317  1.000000  0.214660  0.264891  0.161847   \n",
       "q_004      0.167943  0.057994  0.214660  1.000000  0.170764  0.167594   \n",
       "q_005      0.226024  0.021039  0.264891  0.170764  1.000000  0.220608   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "q_130_QLD  0.001563  0.012654  0.006910  0.106358  0.064769  0.071818   \n",
       "q_130_SA   0.051621  0.003613  0.091384  0.066461  0.076097  0.060867   \n",
       "q_130_TAS  0.047694  0.013221  0.040850  0.079120  0.047702  0.005150   \n",
       "q_130_VIC  0.159945  0.013354  0.153212  0.072732  0.092577  0.127569   \n",
       "q_130_WA   0.036922  0.046457  0.055473  0.063724  0.015221  0.083530   \n",
       "\n",
       "              q_007     q_008     q_009     q_011  ...  q_125_none  \\\n",
       "q_000      0.189307  0.280651  0.301535  0.209196  ...    0.096321   \n",
       "q_001      0.016355  0.015211  0.013852  0.051934  ...    0.028142   \n",
       "q_003      0.190428  0.180217  0.280224  0.237975  ...    0.105369   \n",
       "q_004      0.122863  0.215559  0.139821  0.122239  ...    0.082332   \n",
       "q_005      0.153931  0.175262  0.159404  0.233553  ...    0.089531   \n",
       "...             ...       ...       ...       ...  ...         ...   \n",
       "q_130_QLD  0.043102  0.039825  0.067799  0.067677  ...    0.035946   \n",
       "q_130_SA   0.023131  0.000675  0.016460  0.011996  ...    0.020909   \n",
       "q_130_TAS  0.011674  0.029653  0.068186  0.065537  ...    0.012326   \n",
       "q_130_VIC  0.124956  0.210926  0.241464  0.138426  ...    0.077269   \n",
       "q_130_WA   0.092971  0.074990  0.062237  0.013202  ...    0.020095   \n",
       "\n",
       "           q_125_other  q_130_ACT  q_130_NSW  q_130_NT  q_130_QLD  q_130_SA  \\\n",
       "q_000         0.008710   0.074067   0.083569  0.025828   0.001563  0.051621   \n",
       "q_001         0.029730   0.001382   0.012564  0.002130   0.012654  0.003613   \n",
       "q_003         0.006056   0.039339   0.065728  0.054312   0.006910  0.091384   \n",
       "q_004         0.036288   0.036437   0.037971  0.044416   0.106358  0.066461   \n",
       "q_005         0.042583   0.019635   0.029016  0.042271   0.064769  0.076097   \n",
       "...                ...        ...        ...       ...        ...       ...   \n",
       "q_130_QLD     0.018639   0.062548   0.371575  0.096443   1.000000  0.163595   \n",
       "q_130_SA      0.020909   0.036383   0.216135  0.056098   0.163595  1.000000   \n",
       "q_130_TAS     0.012326   0.021448   0.127416  0.033071   0.096443  0.056098   \n",
       "q_130_VIC     0.034503   0.060037   0.356655  0.092570   0.269957  0.157026   \n",
       "q_130_WA      0.020095   0.034966   0.207721  0.053914   0.157226  0.091454   \n",
       "\n",
       "           q_130_TAS  q_130_VIC  q_130_WA  \n",
       "q_000       0.047694   0.159945  0.036922  \n",
       "q_001       0.013221   0.013354  0.046457  \n",
       "q_003       0.040850   0.153212  0.055473  \n",
       "q_004       0.079120   0.072732  0.063724  \n",
       "q_005       0.047702   0.092577  0.015221  \n",
       "...              ...        ...       ...  \n",
       "q_130_QLD   0.096443   0.269957  0.157226  \n",
       "q_130_SA    0.056098   0.157026  0.091454  \n",
       "q_130_TAS   1.000000   0.092570  0.053914  \n",
       "q_130_VIC   0.092570   1.000000  0.150913  \n",
       "q_130_WA    0.053914   0.150913  1.000000  \n",
       "\n",
       "[91 rows x 91 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = X_train_scaled_df.corr().abs()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated variable pairs: [('q_123_regional', 'q_123_urban'), ('q_125_female', 'q_125_male')]\n"
     ]
    }
   ],
   "source": [
    "# Select highly correlated features\n",
    "high_corr_threshold = 0.5  # This threshold can be adjusted\n",
    "high_corr_var = [(c1, c2) for c1 in corr_matrix.columns for c2 in corr_matrix.columns if c1 < c2 and corr_matrix.at[c1, c2] > high_corr_threshold]\n",
    "print(\"Highly correlated variable pairs:\", high_corr_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
